<!DOCTYPE html><html>

<head>
<meta charset="utf-8">
<title>all</title>
<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>
</head>
<body>


<h1 id="toc_0">RHEL HA Workshop</h1>

<p><strong>Author</strong>: Pat Harrison &lt;<a href="mailto:pharriso@redhat.com">pharriso@redhat.com</a>&gt;</p>

<h2 id="toc_2">Workshop Agenda</h2>

<h3 id="toc_3">Install and Configure</h3>

<table>
<thead>
<tr>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>Create Basic Cluster</td>
</tr>
<tr>
<td>Configure Power Fencing</td>
</tr>
<tr>
<td>Prepare cluster resources</td>
</tr>
<tr>
<td>Create and Manage Cluster Resources</td>
</tr>
<tr>
<td>Access pcsd Web UI</td>
</tr>
<tr>
<td>Adding Nodes</td>
</tr>
<tr>
<td>Active/Active resources</td>
</tr>
</tbody>
</table>

<h3 id="toc_3">Testing</h3>

<table>
<thead>
<tr>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>Offline and Online resources</td>
</tr>
<tr>
<td>Controlled failover testing</td>
</tr>
<tr>
<td>Shutdown and Restart of cluster</td>
</tr>
<tr>
<td>Stop resource outside of cluster</td>
</tr>
<tr>
<td>Resource group failure testing</td>
</tr>
<tr>
<td>Crash cluster node</td>
</tr>
</tbody>
</table>
<h2 id="toc_7">Labs Overview</h2>

<p><center>
<img src="images/RHEL_HA_Networks.png"/>
</center></p>

<h2 id="toc_8">Principles and Conventions of the Labs</h2>

<h3 id="toc_9">Code Entry</h3>

<p>Many of the labs will expect you to type commands into the CLI over a secure shell connection to a set of hosts provided. Each command entry will be highlighted, and where necessary the user we&#39;re expecting you to use will be identified. However, for clarity the conventions are identified below.</p>

<p>This is a command entry box:</p>

<pre><code class="language-none"># uname -a
Linux s01.fab.redhat.com 2.6.32-504.16.2.el6.x86_64 #1 SMP Tue Mar 10 17:01:00 EDT 2015 x86_64 x86_64 x86_64 GNU/Linux</code></pre>

<p>Any commands that we expect you to run as <strong>root</strong> will start with a hash (<strong>#</strong>), for example:</p>

<pre><code class="language-none"># whoami
root</code></pre>

<p>Otherwise, expect the command to start with a dollar sign (<strong>$</strong>), for example:</p>

<pre><code class="language-none">$ whoami
stack

$ sudo su -
# whoami
root</code></pre>

<h1 id="toc_11">RHEL HA Overview</h1>

<h2 id="toc_13">Corosync</h2>

<p></p>

<h1 id="toc_17">Install and Configure</h1>

<h2 id="toc_18">Create a basic cluster</h2>

<p>Install the RHEL HA packages on each node. </p>

<pre><code class="language-none"># yum install pcs pacemaker fence-agents-all</code></pre>

<p>Enable cluster communications through the firewall on each node.</p>

<pre><code class="language-none"># firewall-cmd --permanent --add-service=high-availability
# firewall-cmd --add-service=high-availability
</code></pre>

<p>Set password for the pcs administration account.</p>

<pre><code class="language-none"># echo Redhat123 | passwd --stdin hacluster</code></pre>

<p>Start and enable the pcs daemon on each node.</p>

<pre><code class="language-none"># systemctl enable pcsd && systemctl start pcsd </code></pre>

<p>One one node, authenticate the pcs admin user against each node in the cluster.</p>

<pre><code class="language-none">nodea # pcs cluster auth nodea-priv.example.com nodeb-priv.example.com nodec-priv.example.com</code></pre>

<p>In one node, create and start a cluster called cluster1 consisting of our 3 nodes.</p>

<pre><code class="language-none">nodea # pcs cluster setup --start --enable --name cluster1 nodea-priva.demolab.local nodeb-priv.demolab.local nodec-priv.demolab.local</code></pre>

<p>Verify our cluster has been created succesfully.</p>

<pre><code class="language-none">nodea # pcs status
Cluster name: cluster1
WARNING: no stonith devices and stonith-enabled is not false
Stack: corosync
Current DC: nodea.demolab.local (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 24 10:31:22 2018
Last change: Thu May 24 10:29:00 2018 by hacluster via crmd on nodea.demolab.local

3 nodes configured
0 resources configured

Online: [ nodea.demolab.local nodeb.demolab.local nodec.demolab.local ]

No resources


Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled</code></pre>

<p>Note the warning in the output telling us we haven't enabled fencing. We'll fix that next.</p>

<h2 id="toc_18">Configure Fencing</h2>

<p>We can list all of the available fencing agents.</p>

<pre><code class="language-none">nodea # pcs stonith list</code></pre>

<p>Let's look at all of the available options for the IPMI fencing agent.</p>

<pre><code class="language-none">nodea # pcs stonith describe fence_ipmilan
fence_ipmilan - Fence agent for IPMI

fence_ipmilan is an I/O Fencing agentwhich can be used with machines controlled by IPMI.This agent calls support software ipmitool (http://ipmitool.sf.net/). WARNING! This fence agent might report success before the node is powered off. You should use -m/method onoff if your fence device works correctly with that option.

Stonith options:
  ipport: TCP/UDP port to use for connection with device
  hexadecimal_kg: Hexadecimal-encoded Kg key for IPMIv2 authentication
  port: IP address or hostname of fencing device (together with --port-as-ip)
  inet6_only: Forces agent to use IPv6 addresses only
  ipaddr: IP Address or Hostname
  passwd_script: Script to retrieve password
  method: Method to fence (onoff|cycle)
  inet4_only: Forces agent to use IPv4 addresses only
  passwd: Login password or passphrase
  lanplus: Use Lanplus to improve security of connection
  auth: IPMI Lan Auth type.
  cipher: Ciphersuite to use (same as ipmitool -C parameter)
  target: Bridge IPMI requests to the remote target address
  privlvl: Privilege level on IPMI device
  timeout: Timeout (sec) for IPMI operation
  login: Login Name
  verbose: Verbose mode
  debug: Write debug information to given file
  power_wait: Wait X seconds after issuing ON/OFF
  login_timeout: Wait X seconds for cmd prompt after login
  delay: Wait X seconds before fencing is started
  power_timeout: Test X seconds for status change after ON/OFF
  ipmitool_path: Path to ipmitool binary
  shell_timeout: Wait X seconds for cmd prompt after issuing command
  port_as_ip: Make "port/plug" to be an alias to IP address
  retry_on: Count of attempts to retry power on
  sudo: Use sudo (without password) when calling 3rd party sotfware.
  priority: The priority of the stonith resource. Devices are tried in order of highest priority to lowest.
  pcmk_host_map: A mapping of host names to ports numbers for devices that do not support host names. Eg. node1:1;node2:2,3 would tell the cluster to use port 1 for node1 and ports 2 and 3 for node2
  pcmk_host_list: A list of machines controlled by this device (Optional unless pcmk_host_check=static-list).
  pcmk_host_check: How to determine which machines are controlled by the device. Allowed values: dynamic-list (query the device), static-list (check the pcmk_host_list attribute), none (assume every device can
                   fence every machine)
  pcmk_delay_max: Enable a random delay for stonith actions and specify the maximum of random delay. This prevents double fencing when using slow devices such as sbd. Use this to enable a random delay for
                  stonith actions. The overall delay is derived from this random delay value adding a static delay so that the sum is kept below the maximum delay.
  pcmk_delay_base: Enable a base delay for stonith actions and specify base delay value. This prevents double fencing when different delays are configured on the nodes. Use this to enable a static delay for
                   stonith actions. The overall delay is derived from a random delay value adding this static delay so that the sum is kept below the maximum delay.
  pcmk_action_limit: The maximum number of actions can be performed in parallel on this device Pengine property concurrent-fencing=true needs to be configured first. Then use this to specify the maximum number
                     of actions can be performed in parallel on this device. -1 is unlimited.

Default operations:
  monitor: interval=60s
</code></pre>

<p>We are using KVM virtualisation so we will use the virtual machine fencing agent. This agent talks back to the hypervisor to power machines on/off. First let's check that we can see all of the available VM's</p>

<pre><code class="language-none">nodea # fence_xvm -o list
nodea.example.com              e3d38597-e90c-4bfb-b1d2-144c4ef615b5 on
nodeb.example.com              d3b46128-6df0-4e9d-a7c6-d5bc260a9920 on
nodec.example.com              802a74f3-a533-4e76-8135-267b14a193e7 on
</code></pre>

<p>We can now create the fencing resources in pacemaker.</p>

<pre><code class="language-none">nodea # pcs stonith create nodea-fence fence_xvm pcmk_host_list="nodea.example.com"
nodea # pcs stonith create nodeb-fence fence_xvm pcmk_host_list="nodeb.example.com"
nodea # pcs stonith create nodec-fence fence_xvm pcmk_host_list="nodec.example.com"
</code></pre>

<p>We can confirm the fencing resources are working by running pcs status or pcs stonith.</p>

<pre><code class="language-none">nodea # pcs stonith
 nodea-fence	(stonith:fence_xvm):	Started nodea.demolab.local
 nodeb-fence	(stonith:fence_xvm):	Started nodec.demolab.local
 nodec-fence	(stonith:fence_xvm):	Started nodeb.demolab.local
</code></pre>

<p>Finally, let's test the fencing agent.</p>

<pre><code class="language-none">nodea # pcs stonith fence nodeb.demolab.local</code></pre>

<p>Once the command prompt comes back we confirm the node has restarted. As these are virtual machines they restart quickly so we can follow the restart easily using watch.</p>

<pre><code class="language-none">nodea # watch pcs status 
Cluster name: cluster1
Stack: corosync
Current DC: nodea.demolab.local (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 24 11:55:10 2018
Last change: Thu May 24 11:39:20 2018 by hacluster via crmd on nodeb.demolab.local

3 nodes configured
3 resources configured

Online: [ nodea.demolab.local nodec.demolab.local ]
OFFLINE: [ nodeb.demolab.local ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodea.demolab.local
 nodeb-fence	(stonith:fence_xvm):	Started nodec.demolab.local
 nodec-fence	(stonith:fence_xvm):	Started nodea.demolab.local

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
</code></pre>

<p>The node should go OFFLINE before re-joining the cluster.</p>

<h2 id="toc_18">Prepare Cluster Resources</h2>

<p>We are going to configure a basic resource group which consists of the following:</p>

<ol>
<li>HA-LVM<br><br></li>
<li>Filesystem<br><br></li>
<li>Apache Web server<br><br></li>
<li>Virtual IP<br><br></li>
</ol>

<h3 id="toc_18">LVM and Filesystem</h2>

<p>First we need to discover the iSCSI targets and the login. We need to do this on all nodes.</p>

<pre><code class="language-none"># iscsiadm --mode discoverydb --type sendtargets --portal iscsi.example.com --discover
# iscsiadm --mode node --targetname iqn.1994-05.com.redhat:iscsi-target --portal iscsi.example.com --login
</code></pre>

<p>dmesg should confirm we have discovered a disk - sda</p>


<pre><code class="language-none">[378055.438294] sd 2:0:0:0: [sda] 41934848 512-byte logical blocks: (21.4 GB/19.9 GiB)
[378055.438576] sd 2:0:0:0: [sda] Write Protect is off
[378055.438579] sd 2:0:0:0: [sda] Mode Sense: 43 00 10 08
[378055.438685] sd 2:0:0:0: [sda] Write cache: enabled, read cache: enabled, supports DPO and FUA
[378055.444792] sd 2:0:0:0: [sda] Attached SCSI disk</code></pre>

<p>On one node let's configure the LVM volume.</p>

<pre><code class="language-none">nodea # pvcreate /dev/sda
nodea # vgcreate ha_vg /dev/sda
nodea # lvcreate -L 5G -n ha_lv ha_vg
nodea # mkfs.xfs /dev/ha_vg/ha_lv
</code></pre>


<p>Now on each node we need to configure exclusive activation of a LVM volume group.</p>

<pre><code class="language-none"># lvmconf --enable-halvm --services --startstopservices</code></pre>

<p>Next we need ensure the local volume groups will still be activated outside of the cluster. Edit /etc/lvm/lvm.conf and add the following line.</p>

<pre><code class="language-none">volume_list = [ "rhel" ]</code></pre>

<p>Rebuild the ramdisk on each node to ensure the nodes will only activate their local volume group and not the cluster volume groups.</p>

<pre><code class="language-none"># dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)
# reboot
</code></pre>

<h3 id="toc_18">Apache</h2>

<p>Now let's install apache on all nodes and enable http traffic through the firewall.</p>

<pre><code class="language-none"># yum -y install httpd
# firewall-cmd --add-service=http --permanent
# firewall-cmd --add-service=http
</code></pre>

<h2 id="toc_18">Create and Manage Cluster Resources</h2>

<h3 id="toc_18">Creating Resources</h2>

<p>We can list all available resource types:</p>

<pre><code class="language-none"># pcs resource list</code></pre>

<p>To list the options for a particular resource:</p>

<pre><code class="language-none"># pcs resource describe LVM
Assumed agent name 'ocf:heartbeat:LVM' (deduced from 'LVM')
ocf:heartbeat:LVM - Controls the availability of an LVM Volume Group

Resource script for LVM. It manages an Linux Volume Manager volume (LVM) 
as an HA resource.

Resource options:
  volgrpname (required): The name of volume group.
  exclusive: If set, the volume group will be activated exclusively. This option works one of two ways. If the volume group has the cluster attribute
             set, then the volume group will be activated exclusively using clvmd across the cluster. If the cluster attribute is not set, the volume
             group will be activated exclusively through the use of the volume_list filter in lvm.conf. In the filter scenario, the LVM agent
             verifies that pacemaker's configuration will result in the volume group only being active on a single node in the cluster and that the
             local node's volume_list filter will prevent the volume group from activating outside of the resource agent. On activation this agent
             claims the volume group through the use of a unique tag, and then overrides the volume_list field in a way that allows the volume group
             to be activated only by the agent. To use exclusive activation without clvmd, the volume_list in lvm.conf must be initialized. If volume
             groups exist locally that are not controlled by the cluster, such as the root volume group, make sure those volume groups are listed in
             the volume_list so they will be allowed to activate on bootup.
  tag: If "exclusive" is set on a non clustered volume group, this overrides the tag to be used.
  partial_activation: If set, the volume group will be activated even only partial of the physical volumes available. It helps to set to true, when
                      you are using mirroring logical volumes.

Default operations:
  start: interval=0s timeout=30
  stop: interval=0s timeout=30
  monitor: interval=10 timeout=30
  methods: interval=0s timeout=5
</code></pre>

<p>Now we are going to create a resource group consisting of the following resources:</p>

<ol>
<li>HA-LVM<br><br></li>
<li>Filesystem<br><br></li>
<li>Apache Web server<br><br></li>
<li>Virtual IP<br><br></li>
</ol>
<p>Resources added to a resource group and started in the order they are added. First let's add the HA-LVM resource to activate the volume group.</p>

<pre><code class="language-none"># pcs resource create ha_vg LVM volgrpname=ha_vg exclusive=true --group webapp</code></pre>

<p>Next we need to add a filesystem. We will mount our filesystem at /var/www/html which is our document root.</p>

<pre><code class="language-none"># pcs resource create www_filesystem filesystem device=/dev/ha_vg/ha_lv directory=/var/www/html fstype=xfs --group webapp</code></pre>

<p>The next resource we will add will be the virtual IP.</p>

<pre><code class="language-none"># pcs resource create www_vip IPaddr2 ip=10.50.0.57 --group webapp</code></pre>

<p>The final resource we need is apache.</p>

<pre><code class="language-none"># pcs resource create www_app apache configfile=/etc/httpd/conf/httpd.conf --group webapp</code></pre>

<p>We should now be able to access our website on it's virtual IP address.</p>

<h3 id="toc_18">Managing Resources</h2>

<p>Let's quickly test if we can move our resource to a different node. First let's see what node it is running on.</p>

<pre><code class="language-none"># pcs status
Cluster name: cluster1
Stack: corosync
Current DC: nodea.demolab.local (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Mon May 28 21:19:29 2018
Last change: Mon May 28 21:15:54 2018 by root via crm_resource on nodea.demolab.local

3 nodes configured
7 resources configured

Online: [ nodea.demolab.local nodeb.demolab.local nodec.demolab.local ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodea.demolab.local
 nodeb-fence	(stonith:fence_xvm):	Started nodec.demolab.local
 nodec-fence	(stonith:fence_xvm):	Started nodeb.demolab.local
 Resource Group: webapp
     ha_vg	(ocf::heartbeat:LVM):	Started nodea.demolab.local
     www_filesystem	(ocf::heartbeat:Filesystem):	Started nodea.demolab.local
     www_vip	(ocf::heartbeat:IPaddr2):	Started nodea.demolab.local
     www_app	(ocf::heartbeat:apache):	Started nodea.demolab.local

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
</code></pre>

<p>Now let's move the resource group. We will come back to the message about location constraints.</p>

<pre><code class="language-none"># pcs resource move webapp
Warning: Creating location constraint cli-ban-webapp-on-nodea.demolab.local with a score of -INFINITY for resource webapp on node nodea.demolab.local.
This will prevent webapp from running on nodea.demolab.local until the constraint is removed. This will be the case even if nodea.demolab.local is the last node in the cluster.
</code></pre>

<p>Now check if the resources are running on a different node.</p>

<p>So what was the message about constraints? When we move a resource it creates a constraint which prevents the resource group from moving back to that node. This constraint can be viewed and cleared as follows:</p>

<pre><code class="language-none"># pcs constraint
Location Constraints:
  Resource: webapp
    Disabled on: nodea.demolab.local (score:-INFINITY) (role: Started)
Ordering Constraints:
Colocation Constraints:
Ticket Constraints:
</code></pre>

<p>To clear the constraint we can run the following.</p>
<pre><code class="language-none"># pcs resource clear webapp</code></pre>

<h4 id="toc_18">Stopping, starting & restarting Resources</h2>
<p>The following commands stop, start and restart a resource.</p>
<pre><code class="language-none"># pcs resource disable www_app
# pcs resource enable www_app
# pcs resource restart www_app
</code></pre>

<h4 id="toc_18">Unmanaging resources</h4>
<p>Sometimes it can be useful to be able to stop and start resources outside of the clusters control. Unmanaging a resource stops pcs from actively manaing a resource. The below example would allow us to stop httpd without the cluster taking any action.</p>

<pre><code class="language-none"># pcs resource unmanage www_app
# systemctl stop httpd
# pcs resource manage www_app
</code></pre>

<h3 id="toc_18">Managing Nodes</h2>

<p>Sometimes we need to perform maintenance on nodes and need to either stop them from running resources or particular resources. Placing a node in standby stops it from running any resources.</p>

<pre><code class="language-none"># pcs cluster standby nodea.example.com</code></pre>

<p>pcs status will confirm the node is now in standby. If it was running the cluster resources they will have moved to another node.</p>


<pre><code class="language-none"># pcs status
Cluster name: cluster1
Stack: corosync
Current DC: nodea.demolab.local (version 1.1.18-11.el7_5.2-2b07d5c5a9) - partition with quorum
Last updated: Thu May 31 15:11:14 2018
Last change: Thu May 31 15:11:05 2018 by root via cibadmin on nodea.demolab.local

3 nodes configured
7 resources configured

Node nodea.demolab.local: standby
Online: [ nodeb.demolab.local nodec.demolab.local ]

Full list of resources:

 nodea-fence	(stonith:fence_xvm):	Started nodeb.demolab.local
 nodeb-fence	(stonith:fence_xvm):	Started nodec.demolab.local
 nodec-fence	(stonith:fence_xvm):	Started nodeb.demolab.local
 Resource Group: webapp
     ha_vg	(ocf::heartbeat:LVM):	Started nodec.demolab.local
     www_filesystem	(ocf::heartbeat:Filesystem):	Started nodec.demolab.local
     www_vip	(ocf::heartbeat:IPaddr2):	Started nodec.demolab.local
     www_app	(systemd:httpd):	Starting nodec.demolab.local


Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
</code></pre>

<p>To enable the node to run resources again we need to unstandby it.</p>

<pre><code class="language-none"># pcs cluster unstandby nodea.example.com</code></pre>

<p>Banning a resource just prevents that resource from running on that node. This may be useful if you have multiple resource groups and only want to prevent one of them from running on a node.</p>

<pre><code class="language-none"># pcs resource ban www_app nodec.example.com
# pcs resource clear www_app
</code></pre>

<h3 id="toc_18">Access pcsd Web UI</h2>

<p>The pcs Web UI can be accessed on any of the nodes. For example, browse to https://nodea.example.com:2224 and login with the hacluster credentials we setup when we installed the cluster.</p>

<p><center>
<img src="images/pcs_login.png"/>
</center></p>

<p>Once logged in, we can add our existing cluster. Select "Add Existing" and enter the name of one the nodes in the cluster. Then enter the password for the hacluster user if prompted.

We should now be able to manage the cluster through the web ui. An example of the resources screen can be seen.</p>

<p><center>
<img src="images/pcs_resources.png"/>
</center></p>

<h1 id="toc_17">Adding a node</h1>

<p>The workflow is very similar to the steps we took when creating a new cluster. We will add noded to the cluster. Install the RHEL HA packages on noded.</p>

<pre><code class="language-none">noded # yum install pcs pacemaker fence-agents-all</code></pre>

<p>Enable cluster communications through the firewall on noded.</p>

<pre><code class="language-none">noded # firewall-cmd --permanent --add-service=high-availability
noded # firewall-cmd --add-service=high-availability
</code></pre>

<p>Set password for the pcs administration account.</p>

<pre><code class="language-none">noded # echo Redhat123 | passwd --stdin hacluster</code></pre>

<p>Start and enable the pcs daemon on noded.</p>

<pre><code class="language-none">noded # systemctl enable pcsd && systemctl start pcsd </code></pre>

<p>One an existing node, authenticate the pcs admin user against the new node (noded in our example).</p>

<pre><code class="language-none">nodea # pcs cluster auth noded-priv.example.com</code></pre>

<p>In an existing node, add noded to the cluster.</p>

<pre><code class="language-none">nodea # pcs cluster node add noded-priv.example.com</code></pre>

<p>Finally, on noded we need to start and enable cluster services.</p>

<pre><code class="language-none">noded # pcs cluster start && pcs cluster enable</code></pre>

<p>Verify the new node has been succesfully added with pcs status.</p>
